2022-08-04 10:42:24,975 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask REQUESTING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:42:26,345 - DEBUG [provisioning-task-0:i.c.c.r.s.p.d.DataprocProvisioner@225] - Not checking cluster reuse, enabled: true, skip delete: false, idle ttl: 240, reuse threshold: 3
2022-08-04 10:42:26,908 - INFO  [provisioning-task-0:i.c.c.r.s.p.d.DataprocProvisioner@178] - Creating Dataproc cluster cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a in project qwiklabs-gcp-01-42fcbb83597b, in region us-central1, with image 2.0, with labels {goog-datafusion-version=6_6, cdap-version=6_6_0-1645284843891, goog-datafusion-edition=basic}
2022-08-04 10:42:29,542 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask REQUESTING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:43:44,034 - DEBUG [provisioning-task-1:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:43:44,514 - DEBUG [provisioning-task-1:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:44:16,146 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:44:16,663 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:44:47,482 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:44:48,018 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:45:18,794 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:45:19,268 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:45:49,993 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:45:50,500 - DEBUG [provisioning-task-2:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask POLLING_CREATE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:45:58,406 - DEBUG [provisioning-task-7:i.c.c.i.p.t.ProvisioningTask@116] - Completed PROVISION task for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:46:02,640 - INFO  [program-start-2:i.c.c.i.a.r.d.DistributedProgramRunner@503] - Starting Workflow Program 'DataPipelineWorkflow' with Arguments [logical.start.time=1659609743515, system.profile.name=SYSTEM:dataproc], with debugging false
2022-08-04 10:46:02,764 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@521] - Create and copy application.jar
2022-08-04 10:46:07,771 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@529] - Done application.jar
2022-08-04 10:46:07,774 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@542] - Create and copy resources.jar
2022-08-04 10:46:07,784 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@545] - Done resources.jar
2022-08-04 10:46:07,785 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@585] - Populating Runnable LocalFiles
2022-08-04 10:46:07,787 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/log-appender-ext.zip
2022-08-04 10:46:07,788 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/cConf.xml
2022-08-04 10:46:07,788 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/etc/cdap/conf/logback-container.xml
2022-08-04 10:46:07,788 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/1659609960716-0/artifacts.jar
2022-08-04 10:46:07,788 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/appSpec8195930702198905564.json
2022-08-04 10:46:07,789 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/workflow.default.DemoBQ.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a/program.jar
2022-08-04 10:46:07,789 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/hConf.xml
2022-08-04 10:46:07,791 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/1659609960716-0/artifacts.jar
2022-08-04 10:46:07,792 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@592] - Added file file:/data/tmp/1659609959505-0/program.options1397038243969988801.json
2022-08-04 10:46:07,793 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@595] - Done Runnable LocalFiles
2022-08-04 10:46:07,797 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@613] - Creating /data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/runtime.config.jar3022061041011003849/twillSpec.json
2022-08-04 10:46:07,806 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@633] - Done /data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/runtime.config.jar3022061041011003849/twillSpec.json
2022-08-04 10:46:07,806 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@644] - Creating /data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/runtime.config.jar3022061041011003849/logback-template.xml
2022-08-04 10:46:07,807 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@648] - Done /data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/runtime.config.jar3022061041011003849/logback-template.xml
2022-08-04 10:46:07,808 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@550] - Create and copy runtime.config.jar
2022-08-04 10:46:07,812 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.AbstractRuntimeTwillPreparer@572] - Done runtime.config.jar
2022-08-04 10:46:07,871 - INFO  [runtime-scheduler-2:i.c.c.i.a.r.d.r.RuntimeJobTwillPreparer@98] - JVM properties {logback.configurationFile=logback.xml, twill.container.class.loader=io.cdap.cdap.common.app.MainClassLoader}
2022-08-04 10:46:07,872 - INFO  [runtime-scheduler-2:i.c.c.i.a.r.d.r.RuntimeJobTwillPreparer@88] - Starting runnable DataPipelineWorkflow for runId program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a with job manager.
2022-08-04 10:46:07,873 - DEBUG [runtime-scheduler-2:i.c.c.r.s.r.DataprocRuntimeJobManager@174] - Launching run 1fc8e6b3-13e2-11ed-8c74-9e84527b297a with following configurations: cluster cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a, project qwiklabs-gcp-01-42fcbb83597b, region us-central1, bucket df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa.
2022-08-04 10:46:09,977 - DEBUG [provisioning-context-0:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 248150004 bytes from file:/data/tmp/1659609959505-0/1659609960716-0/artifacts.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/artifacts_archive.jar 
2022-08-04 10:46:09,979 - DEBUG [provisioning-context-1:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 248150004 bytes from file:/data/tmp/1659609959505-0/1659609960716-0/artifacts.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/artifacts 
2022-08-04 10:46:09,987 - DEBUG [provisioning-context-2:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 54423556 bytes from file:/data/tmp/runner.cache5083980983868660346/65206652e8550d44362557de0bb879a6-application.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/application.jar 
2022-08-04 10:46:09,996 - DEBUG [provisioning-context-3:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 22798132 bytes from file:/data/tmp/1659609959505-0/log-appender-ext.zip to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/log-appender-ext 
2022-08-04 10:46:10,001 - DEBUG [provisioning-context-4:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 11549548 bytes from file:/data/tmp/workflow.default.DemoBQ.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a/program.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/program.jar 
2022-08-04 10:46:10,002 - DEBUG [provisioning-context-5:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 10167406 bytes from file:/tmp/dataproc.launcher1829474460409324341/twill.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/twill.jar 
2022-08-04 10:46:10,011 - DEBUG [provisioning-context-6:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 171983 bytes from file:/data/tmp/1659609959505-0/hConf.xml to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/hConf.xml 
2022-08-04 10:46:10,016 - DEBUG [provisioning-context-7:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 102301 bytes from file:/data/tmp/1659609959505-0/appSpec8195930702198905564.json to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/appSpec.json 
2022-08-04 10:46:10,028 - DEBUG [provisioning-context-8:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 73258 bytes from file:/data/tmp/1659609959505-0/cConf.xml to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/cConf.xml 
2022-08-04 10:46:10,031 - DEBUG [provisioning-context-9:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 29064 bytes from file:/data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/resources.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/resources.jar 
2022-08-04 10:46:10,234 - DEBUG [provisioning-context-1:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,236 - DEBUG [provisioning-context-7:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,236 - DEBUG [provisioning-context-0:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,236 - DEBUG [provisioning-context-6:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,236 - DEBUG [provisioning-context-3:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,238 - DEBUG [provisioning-context-4:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,238 - DEBUG [provisioning-context-5:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,238 - DEBUG [provisioning-context-9:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,238 - DEBUG [provisioning-context-8:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,241 - DEBUG [provisioning-context-2:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,441 - DEBUG [provisioning-context-9:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/resources.jar.
2022-08-04 10:46:10,442 - DEBUG [provisioning-context-6:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/hConf.xml.
2022-08-04 10:46:10,451 - DEBUG [provisioning-context-9:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 11284 bytes from file:/tmp/dataproc.launcher1829474460409324341/launcher.jar to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/launcher.jar 
2022-08-04 10:46:10,452 - DEBUG [provisioning-context-6:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 3942 bytes from file:/etc/cdap/conf/logback-container.xml to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/logback.xml 
2022-08-04 10:46:10,463 - DEBUG [provisioning-context-8:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/cConf.xml.
2022-08-04 10:46:10,463 - DEBUG [provisioning-context-8:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 2780 bytes from file:/data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/runtime.config.jar484425156193609695.tmp to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/runtime.config.jar 
2022-08-04 10:46:10,468 - DEBUG [provisioning-context-7:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/appSpec8195930702198905564.json.
2022-08-04 10:46:10,474 - DEBUG [provisioning-context-7:i.c.c.r.s.r.DataprocRuntimeJobManager@326] - Uploading a file of size 2616 bytes from file:/data/tmp/1659609959505-0/program.options1397038243969988801.json to gs://df-8001745278760211721-6wce5gqtxai63doyaizbbqaaaa/cdap-job/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/program.options.json 
2022-08-04 10:46:10,476 - DEBUG [provisioning-context-9:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,487 - DEBUG [provisioning-context-6:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,495 - DEBUG [provisioning-context-8:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,503 - DEBUG [provisioning-context-7:i.c.c.r.s.r.DataprocRuntimeJobManager@330] - File's Location type : region and Location : US-CENTRAL1. 
2022-08-04 10:46:10,577 - DEBUG [provisioning-context-9:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/tmp/dataproc.launcher1829474460409324341/launcher.jar.
2022-08-04 10:46:10,609 - DEBUG [provisioning-context-6:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/etc/cdap/conf/logback-container.xml.
2022-08-04 10:46:10,633 - DEBUG [provisioning-context-7:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/program.options1397038243969988801.json.
2022-08-04 10:46:10,637 - DEBUG [provisioning-context-8:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1fc8e6b3-13e2-11ed-8c74-9e84527b297a7028510319278996156/runtime.config.jar484425156193609695.tmp.
2022-08-04 10:46:10,911 - DEBUG [provisioning-context-5:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/tmp/dataproc.launcher1829474460409324341/twill.jar.
2022-08-04 10:46:11,083 - DEBUG [provisioning-context-4:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/workflow.default.DemoBQ.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a/program.jar.
2022-08-04 10:46:11,332 - DEBUG [provisioning-context-3:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/log-appender-ext.zip.
2022-08-04 10:46:12,440 - DEBUG [provisioning-context-2:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/runner.cache5083980983868660346/65206652e8550d44362557de0bb879a6-application.jar.
2022-08-04 10:46:18,026 - DEBUG [provisioning-context-1:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/1659609960716-0/artifacts.jar.
2022-08-04 10:46:18,521 - DEBUG [provisioning-context-0:i.c.c.r.s.r.DataprocRuntimeJobManager@336] - Successfully Uploaded file : file:/data/tmp/1659609959505-0/1659609960716-0/artifacts.jar.
2022-08-04 10:46:18,741 - DEBUG [runtime-scheduler-2:i.c.c.r.s.r.DataprocRuntimeJobManager@204] - Successfully submitted hadoop job default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a to cluster cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:46:18,745 - DEBUG [runtime-scheduler-2:i.c.c.i.a.r.d.r.RemoteExecutionTwillRunnerService@576] - Startup task completed for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a
2022-08-04 10:46:43,653 - DEBUG [main:i.c.c.i.a.r.m.RuntimeMonitors@136] - Setting runtime service routing base URI to https://r-cdf-lab-instance-qwiklabs-gcp-01-42fcbb83597b-dot-usc1.datafusion.googleusercontent.com/v3Internal/runtime/namespaces/default/apps/DemoBQ/versions/-SNAPSHOT/workflows/DataPipelineWorkflow/runs/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/services/
2022-08-04 10:46:45,323 - INFO  [TMSLogPublisher:i.c.c.m.s.l.LevelDBTableFactory@171] - Messaging metadata table created at data/messaging/namespace:system.tms.meta
2022-08-04 10:46:46,902 - DEBUG [main:i.c.c.i.a.r.d.r.DefaultRuntimeJob@422] - Starting core service LogAppenderLoaderService [NEW]
2022-08-04 10:46:48,617 - INFO  [LogAppenderLoaderService STARTING:i.c.c.l.a.l.LogAppenderLoaderService@63] - Log Appender stackdriver is initialized and started.
2022-08-04 10:46:49,147 - DEBUG [main:i.c.c.i.a.r.d.r.DefaultRuntimeJob@422] - Starting core service CoreMessagingService [NEW]
2022-08-04 10:46:49,177 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.audit
2022-08-04 10:46:49,188 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metadata
2022-08-04 10:46:49,193 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.dataevent
2022-08-04 10:46:49,199 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics0
2022-08-04 10:46:49,209 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics1
2022-08-04 10:46:49,218 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics2
2022-08-04 10:46:49,224 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics3
2022-08-04 10:46:49,229 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics4
2022-08-04 10:46:49,256 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics5
2022-08-04 10:46:49,260 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics6
2022-08-04 10:46:49,269 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics7
2022-08-04 10:46:49,276 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics8
2022-08-04 10:46:49,291 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metrics9
2022-08-04 10:46:49,296 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.metricsadmin
2022-08-04 10:46:49,305 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.timeevent
2022-08-04 10:46:49,308 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.programstatusevent
2022-08-04 10:46:49,318 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.programstatusrecordevent
2022-08-04 10:46:49,321 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs0
2022-08-04 10:46:49,326 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs1
2022-08-04 10:46:49,330 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs2
2022-08-04 10:46:49,338 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs3
2022-08-04 10:46:49,343 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs4
2022-08-04 10:46:49,350 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs5
2022-08-04 10:46:49,356 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs6
2022-08-04 10:46:49,366 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs7
2022-08-04 10:46:49,376 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs8
2022-08-04 10:46:49,381 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.logs9
2022-08-04 10:46:49,390 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.preview
2022-08-04 10:46:49,399 - DEBUG [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@306] - System topic created: topic:system.previewlog0
2022-08-04 10:46:49,405 - INFO  [CoreMessagingService STARTING:i.c.c.m.s.CoreMessagingService@234] - Core Messaging Service started
2022-08-04 10:46:49,412 - DEBUG [main:i.c.c.i.a.r.d.r.DefaultRuntimeJob@422] - Starting core service MessagingHttpService [NEW]
2022-08-04 10:46:49,421 - INFO  [MessagingHttpService STARTING:i.c.h.NettyHttpService@181] - Starting HTTP Service messaging.service at address cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.3:0
2022-08-04 10:46:50,049 - DEBUG [MessagingHttpService STARTING:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@76] - Discoverable messaging.service with address cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.3:40239 added to configuration with payload https://
2022-08-04 10:46:50,059 - DEBUG [MessagingHttpService STARTING:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable messaging.service with address cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.3:40239 and payload https://
2022-08-04 10:46:50,075 - INFO  [MessagingHttpService STARTING:i.c.c.m.s.MessagingHttpService@112] - Messaging HTTP server started on cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.3:40239
2022-08-04 10:46:50,083 - DEBUG [main:i.c.c.i.a.r.d.r.DefaultRuntimeJob@422] - Starting core service MessagingMetricsCollectionService [NEW]
2022-08-04 10:46:50,097 - DEBUG [main:i.c.c.i.a.r.d.r.DefaultRuntimeJob@422] - Starting core service RuntimeClientService [NEW]
2022-08-04 10:46:50,104 - DEBUG [RuntimeClientService:i.c.c.c.s.AbstractRetryableScheduledService@99] - Starting scheduled service RuntimeClientService
2022-08-04 10:46:50,112 - DEBUG [main:i.c.c.i.a.r.d.r.DefaultRuntimeJob@422] - Starting core service ProfileMetricService [NEW]
2022-08-04 10:46:50,204 - DEBUG [main:i.c.c.a.r.s.SparkProgramRuntimeProvider@143] - using sparkCompat SPARK2_2_11
2022-08-04 10:46:50,315 - DEBUG [RuntimeClientService:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable runtime with address r-cdf-lab-instance-qwiklabs-gcp-01-42fcbb83597b-dot-usc1.datafusion.googleusercontent.com/142.250.148.132:443 and payload https://
2022-08-04 10:46:55,429 - WARN  [main:i.c.c.c.c.Configuration@1814] - file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/cConf.xml:an attempt to override final parameter: messaging.system.topics;  Ignoring.
2022-08-04 10:46:55,438 - WARN  [main:i.c.c.c.c.Configuration@1814] - file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/cConf.xml:an attempt to override final parameter: messaging.max.instances;  Ignoring.
2022-08-04 10:46:55,455 - WARN  [main:i.c.c.c.c.Configuration@1814] - file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/cConf.xml:an attempt to override final parameter: app.program.runtime.monitor.server.info.file;  Ignoring.
2022-08-04 10:46:55,507 - DEBUG [main:i.c.c.a.r.s.SparkProgramRuntimeProvider@143] - using sparkCompat SPARK2_2_11
2022-08-04 10:46:55,747 - DEBUG [main:i.c.c.a.r.s.SparkPackageUtils@314] - Located Spark library in in /usr/lib/spark/jars
2022-08-04 10:46:58,892 - DEBUG [main:i.c.c.a.r.s.SparkPackageUtils@314] - Located Spark library in in /usr/lib/spark/jars
2022-08-04 10:47:11,744 - DEBUG [main:i.c.c.a.r.s.SparkPackageUtils@256] - Adding files from /etc/hadoop/conf to __spark_conf__.zip
2022-08-04 10:47:12,004 - DEBUG [main:i.c.c.i.a.r.d.DistributedProgramRunner@389] - Configure log appender provider to stackdriver
2022-08-04 10:47:14,516 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: pyspark.zip LocalizeResource{archive=false, uri=file:/usr/lib/spark/python/lib/pyspark.zip}
2022-08-04 10:47:14,526 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: logback.xml LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/logback.xml}
2022-08-04 10:47:14,530 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: __spark_conf__ LocalizeResource{archive=true, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/__spark_conf__5154789389137921028.zip}
2022-08-04 10:47:14,534 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: appSpec.json LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/appSpec3851680066465797890.json}
2022-08-04 10:47:14,538 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: hConf.xml LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/hConf.xml}
2022-08-04 10:47:14,541 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: program.options.json LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/program.options6667928530559902075.json}
2022-08-04 10:47:14,545 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: spark.archive-spark2_2.11-3.2.3.zip LocalizeResource{archive=true, uri=hdfs://cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m/framework/spark/spark.archive-spark2_2.11-3.2.3.zip}
2022-08-04 10:47:14,549 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: log-appender-ext LocalizeResource{archive=true, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/log-appender-ext.zip}
2022-08-04 10:47:14,552 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: cConf.xml LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/cConf.xml}
2022-08-04 10:47:14,555 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: artifacts_archive.jar LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/artifacts_archive.jar}
2022-08-04 10:47:14,559 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: spark-defaults.conf LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0/spark-defaults.conf2985119537278733575.tmp}
2022-08-04 10:47:14,561 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: program.jar LocalizeResource{archive=false, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/program.jar}
2022-08-04 10:47:14,565 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: py4j-0.10.9-src.zip LocalizeResource{archive=false, uri=file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip}
2022-08-04 10:47:14,567 - DEBUG [main:i.c.c.i.a.r.d.ProgramTwillApplication@126] - Localizing file for program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a: artifacts LocalizeResource{archive=true, uri=file:/tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/artifacts_archive.jar}
2022-08-04 10:47:14,708 - INFO  [main:i.c.c.i.a.r.d.DistributedProgramRunner@503] - Starting Workflow Program 'DataPipelineWorkflow' with Arguments [logical.start.time=1659609743515, system.profile.name=SYSTEM:dataproc], with debugging false
2022-08-04 10:47:32,833 - INFO  [ STARTING:o.a.t.y.YarnTwillController@124] - Application workflow.default.DemoBQ.DataPipelineWorkflow with id application_1659609855541_0001 submitted
2022-08-04 10:47:46,364 - INFO  [ STARTING:o.a.t.y.YarnTwillController@136] - Yarn application workflow.default.DemoBQ.DataPipelineWorkflow application_1659609855541_0001 is in state RUNNING
2022-08-04 10:47:46,387 - DEBUG [main:i.c.c.i.a.r.d.DistributedProgramRunner@656] - Cleanup tmp files for program:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow: /tmp/default_DemoBQ_DataPipelineWorkflow_1fc8e6b3-13e2-11ed-8c74-9e84527b297a/data/tmp/1659610015464-0
2022-08-04 10:47:46,404 - INFO  [main:i.c.c.i.a.r.d.AbstractTwillProgramController@67] - Twill program running: program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a, twill runId: 7f5933f1-d3aa-4ea5-8def-c19032b8341e
2022-08-04 10:48:10,133 - DEBUG [TwillContainerService:i.c.c.i.a.r.m.RuntimeMonitors@136] - Setting runtime service routing base URI to https://r-cdf-lab-instance-qwiklabs-gcp-01-42fcbb83597b-dot-usc1.datafusion.googleusercontent.com/v3Internal/runtime/namespaces/default/apps/DemoBQ/versions/-SNAPSHOT/workflows/DataPipelineWorkflow/runs/1fc8e6b3-13e2-11ed-8c74-9e84527b297a/services/
2022-08-04 10:48:10,437 - DEBUG [TMSLogPublisher:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable messaging.service with address cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.3:40239 and payload https://
2022-08-04 10:48:15,659 - INFO  [TwillContainerService:i.c.c.i.a.r.d.AbstractProgramTwillRunnable@158] - Runnable initialized: DataPipelineWorkflow
2022-08-04 10:48:16,588 - INFO  [LogAppenderLoaderService STARTING:i.c.c.l.a.l.LogAppenderLoaderService@63] - Log Appender stackdriver is initialized and started.
2022-08-04 10:48:16,819 - INFO  [TwillContainerService:i.c.c.i.a.r.d.AbstractProgramTwillRunnable@244] - Starting program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a
2022-08-04 10:48:20,568 - INFO  [WorkflowDriver:i.c.c.d.SmartWorkflow@470] - Pipeline 'DemoBQ' is started by user 'yarn' with arguments {logical.start.time=1659609743515, system.profile.name=SYSTEM:dataproc}
2022-08-04 10:48:20,681 - INFO  [WorkflowDriver:i.c.c.d.SmartWorkflow@505] - Pipeline 'DemoBQ' running
2022-08-04 10:48:20,743 - DEBUG [WorkflowDriver:i.c.c.i.a.r.w.WorkflowProgramController@71] - Workflow service workflow.default.DemoBQ.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a started
2022-08-04 10:48:20,748 - INFO  [WorkflowDriver:i.c.c.i.a.r.w.WorkflowDriver@623] - Starting workflow execution for 'DataPipelineWorkflow' with Run id '1fc8e6b3-13e2-11ed-8c74-9e84527b297a'
2022-08-04 10:48:20,807 - DEBUG [action-phase-1-0:i.c.c.a.r.s.SparkProgramRuntimeProvider@143] - using sparkCompat SPARK2_2_11
2022-08-04 10:48:21,028 - DEBUG [action-phase-1-0:i.c.c.a.r.s.SparkPackageUtils@314] - Located Spark library in in spark.archive-spark2_2.11-3.2.3.zip
2022-08-04 10:48:21,225 - DEBUG [action-phase-1-0:i.c.c.c.g.DFSLocationModule@76] - Location namespace is /cdap
2022-08-04 10:48:21,231 - DEBUG [action-phase-1-0:i.c.c.c.g.FileContextProvider@68] - Getting filesystem for user yarn
2022-08-04 10:48:21,235 - DEBUG [action-phase-1-0:i.c.c.c.g.DFSLocationModule@94] - Location cache path is data/location.cache
2022-08-04 10:48:21,285 - INFO  [action-phase-1-0:i.c.c.i.a.r.w.WorkflowDriver@337] - Starting Spark Program 'phase-1' in workflow
2022-08-04 10:48:21,750 - DEBUG [action-phase-1-0:i.c.c.a.r.s.SparkProgramRunner@219] - Starting Spark Job. Context: SparkRuntimeContext{id=program:default.DemoBQ.-SNAPSHOT.spark.phase-1, runId=f506beb1-13e2-11ed-8877-42010a800005}
2022-08-04 10:48:26,177 - WARN  [SparkRunnerphase-1:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:48:26,374 - DEBUG [SparkRunnerphase-1:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable svc.system.pipeline.studio with address svc.system.pipeline.studio:0 and payload https://
2022-08-04 10:48:26,883 - DEBUG [SparkRunnerphase-1:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable dataset.service with address dataset.service:0 and payload https://
2022-08-04 10:48:28,449 - WARN  [SparkRunnerphase-1:o.a.h.f.FileSystem@3304] - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HftpFileSystem not found
2022-08-04 10:48:28,457 - WARN  [SparkRunnerphase-1:o.a.h.f.FileSystem@3304] - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HsftpFileSystem not found
2022-08-04 10:48:32,613 - INFO  [SparkExecutionService STARTING:i.c.h.NettyHttpService@181] - Starting HTTP Service phase-1-spark-exec-service at address cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-w-1.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.5:0
2022-08-04 10:48:33,117 - DEBUG [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:i.c.c.a.r.s.s.AbstractSparkSubmitter@164] - Calling SparkSubmit for program:default.DemoBQ.-SNAPSHOT.spark.phase-1 f506beb1-13e2-11ed-8877-42010a800005: [--master, yarn, --deploy-mode, cluster, --conf, spark.app.name=phase-1, --conf, spark.yarn.am.memory=640m, --conf, spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2, --conf, spark.driver.maxResultSize=512m, --conf, spark.sql.catalogImplementation=hive, --conf, spark.executor.extraClassPath=$PWD/cdap-spark-launcher.jar:$PWD/logback.xml.jar:$PWD/cdap-spark.jar/aopalliance.aopalliance-1.0.jar:$PWD/cdap-spark.jar/ch.qos.logback.logback-classic-1.0.9.jar:$PWD/cdap-spark.jar/ch.qos.logback.logback-core-1.0.9.jar:$PWD/cdap-spark.jar/com.101tec.zkclient-0.3.jar:$PWD/cdap-spark.jar/com.google.code.findbugs.jsr305-2.0.1.jar:$PWD/cdap-spark.jar/com.google.code.gson.gson-2.2.4.jar:$PWD/cdap-spark.jar/com.google.guava.guava-13.0.1.jar:$PWD/cdap-spark.jar/com.google.inject.extensions.guice-assistedinject-4.0.jar:$PWD/cdap-spark.jar/com.google.inject.extensions.guice-multibindings-4.0.jar:$PWD/cdap-spark.jar/com.google.inject.guice-4.0.jar:$PWD/cdap-spark.jar/commons-beanutils.commons-beanutils-1.7.0.jar:$PWD/cdap-spark.jar/commons-compiler-3.0.16.jar:$PWD/cdap-spark.jar/commons-io.commons-io-2.4.jar:$PWD/cdap-spark.jar/error_prone_annotations-2.3.4.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-api-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-api-common-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-api-spark2_2.11-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-app-fabric-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-common-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-data-fabric-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-explore-client-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-formats-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-hbase-compat-base-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-hbase-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-master-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-metadata-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-proto-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-runtime-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-security-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-security-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-spark-core2_2.11-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-spark-python-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-storage-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-system-app-api-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-tms-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-watchdog-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-watchdog-api-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.common.common-http-0.13.0.jar:$PWD/cdap-spark.jar/io.cdap.common.common-io-0.13.0.jar:$PWD/cdap-spark.jar/io.cdap.http.netty-http-1.7.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-api-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-common-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-core-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-discovery-api-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-discovery-core-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-yarn-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-zookeeper-1.0.0.jar:$PWD/cdap-spark.jar/io.dropwizard.metrics.metrics-core-3.1.2.jar:$PWD/cdap-spark.jar/io.netty.netty-all-4.1.16.Final.jar:$PWD/cdap-spark.jar/it.unimi.dsi.fastutil-6.5.6.jar:$PWD/cdap-spark.jar/jackson-core-asl-1.9.13.jar:$PWD/cdap-spark.jar/jackson-mapper-asl-1.9.13.jar:$PWD/cdap-spark.jar/janino-3.0.16.jar:$PWD/cdap-spark.jar/javax.inject.javax.inject-1.jar:$PWD/cdap-spark.jar/javax.ws.rs.javax.ws.rs-api-2.0.jar:$PWD/cdap-spark.jar/log4j.log4j-1.2.17.jar:$PWD/cdap-spark.jar/net.sf.jopt-simple.jopt-simple-3.2.jar:$PWD/cdap-spark.jar/org.apache.avro.avro-1.8.2.jar:$PWD/cdap-spark.jar/org.apache.commons.commons-compress-1.18.jar:$PWD/cdap-spark.jar/org.apache.commons.commons-dbcp2-2.6.0.jar:$PWD/cdap-spark.jar/org.apache.commons.commons-pool2-2.6.1.jar:$PWD/cdap-spark.jar/org.apache.tephra.tephra-api-0.15.0-incubating.jar:$PWD/cdap-spark.jar/org.apache.tephra.tephra-core-0.15.0-incubating.jar:$PWD/cdap-spark.jar/org.apache.thrift.libthrift-0.9.3.jar:$PWD/cdap-spark.jar/org.bouncycastle.bcpkix-jdk15on-1.60.jar:$PWD/cdap-spark.jar/org.bouncycastle.bcprov-jdk15on-1.60.jar:$PWD/cdap-spark.jar/org.iq80.leveldb.leveldb-0.12-uber.jar:$PWD/cdap-spark.jar/org.ow2.asm.asm-7.1.jar:$PWD/cdap-spark.jar/org.ow2.asm.asm-commons-7.1.jar:$PWD/cdap-spark.jar/org.ow2.asm.asm-tree-7.1.jar:$PWD/cdap-spark.jar/org.quartz-scheduler.quartz-2.2.0.jar:$PWD/cdap-spark.jar/org.slf4j.jcl-over-slf4j-1.7.5.jar:$PWD/cdap-spark.jar/org.slf4j.jul-to-slf4j-1.7.5.jar:$PWD/cdap-spark.jar/org.slf4j.slf4j-api-1.7.5.jar:$PWD/cdap-spark.jar/protobuf-java-2.5.0.jar:$PWD/cdap-spark.jar/zookeeper-3.4.6.jar:, --conf, spark.network.timeout=600s, --conf, spark.driver.extraClassPath=$PWD/cdap-spark-launcher.jar:$PWD/logback.xml.jar:$PWD/cdap-spark.jar/aopalliance.aopalliance-1.0.jar:$PWD/cdap-spark.jar/ch.qos.logback.logback-classic-1.0.9.jar:$PWD/cdap-spark.jar/ch.qos.logback.logback-core-1.0.9.jar:$PWD/cdap-spark.jar/com.101tec.zkclient-0.3.jar:$PWD/cdap-spark.jar/com.google.code.findbugs.jsr305-2.0.1.jar:$PWD/cdap-spark.jar/com.google.code.gson.gson-2.2.4.jar:$PWD/cdap-spark.jar/com.google.guava.guava-13.0.1.jar:$PWD/cdap-spark.jar/com.google.inject.extensions.guice-assistedinject-4.0.jar:$PWD/cdap-spark.jar/com.google.inject.extensions.guice-multibindings-4.0.jar:$PWD/cdap-spark.jar/com.google.inject.guice-4.0.jar:$PWD/cdap-spark.jar/commons-beanutils.commons-beanutils-1.7.0.jar:$PWD/cdap-spark.jar/commons-compiler-3.0.16.jar:$PWD/cdap-spark.jar/commons-io.commons-io-2.4.jar:$PWD/cdap-spark.jar/error_prone_annotations-2.3.4.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-api-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-api-common-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-api-spark2_2.11-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-app-fabric-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-common-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-data-fabric-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-explore-client-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-formats-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-hbase-compat-base-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-hbase-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-master-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-metadata-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-proto-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-runtime-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-security-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-security-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-spark-core2_2.11-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-spark-python-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-storage-spi-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-system-app-api-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-tms-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-watchdog-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.cdap.cdap-watchdog-api-6.6.0.jar:$PWD/cdap-spark.jar/io.cdap.common.common-http-0.13.0.jar:$PWD/cdap-spark.jar/io.cdap.common.common-io-0.13.0.jar:$PWD/cdap-spark.jar/io.cdap.http.netty-http-1.7.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-api-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-common-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-core-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-discovery-api-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-discovery-core-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-yarn-1.0.0.jar:$PWD/cdap-spark.jar/io.cdap.twill.twill-zookeeper-1.0.0.jar:$PWD/cdap-spark.jar/io.dropwizard.metrics.metrics-core-3.1.2.jar:$PWD/cdap-spark.jar/io.netty.netty-all-4.1.16.Final.jar:$PWD/cdap-spark.jar/it.unimi.dsi.fastutil-6.5.6.jar:$PWD/cdap-spark.jar/jackson-core-asl-1.9.13.jar:$PWD/cdap-spark.jar/jackson-mapper-asl-1.9.13.jar:$PWD/cdap-spark.jar/janino-3.0.16.jar:$PWD/cdap-spark.jar/javax.inject.javax.inject-1.jar:$PWD/cdap-spark.jar/javax.ws.rs.javax.ws.rs-api-2.0.jar:$PWD/cdap-spark.jar/log4j.log4j-1.2.17.jar:$PWD/cdap-spark.jar/net.sf.jopt-simple.jopt-simple-3.2.jar:$PWD/cdap-spark.jar/org.apache.avro.avro-1.8.2.jar:$PWD/cdap-spark.jar/org.apache.commons.commons-compress-1.18.jar:$PWD/cdap-spark.jar/org.apache.commons.commons-dbcp2-2.6.0.jar:$PWD/cdap-spark.jar/org.apache.commons.commons-pool2-2.6.1.jar:$PWD/cdap-spark.jar/org.apache.tephra.tephra-api-0.15.0-incubating.jar:$PWD/cdap-spark.jar/org.apache.tephra.tephra-core-0.15.0-incubating.jar:$PWD/cdap-spark.jar/org.apache.thrift.libthrift-0.9.3.jar:$PWD/cdap-spark.jar/org.bouncycastle.bcpkix-jdk15on-1.60.jar:$PWD/cdap-spark.jar/org.bouncycastle.bcprov-jdk15on-1.60.jar:$PWD/cdap-spark.jar/org.iq80.leveldb.leveldb-0.12-uber.jar:$PWD/cdap-spark.jar/org.ow2.asm.asm-7.1.jar:$PWD/cdap-spark.jar/org.ow2.asm.asm-commons-7.1.jar:$PWD/cdap-spark.jar/org.ow2.asm.asm-tree-7.1.jar:$PWD/cdap-spark.jar/org.quartz-scheduler.quartz-2.2.0.jar:$PWD/cdap-spark.jar/org.slf4j.jcl-over-slf4j-1.7.5.jar:$PWD/cdap-spark.jar/org.slf4j.jul-to-slf4j-1.7.5.jar:$PWD/cdap-spark.jar/org.slf4j.slf4j-api-1.7.5.jar:$PWD/cdap-spark.jar/protobuf-java-2.5.0.jar:$PWD/cdap-spark.jar/zookeeper-3.4.6.jar:, --conf, spark.sql.adaptive.coalescePartitions.initialPartitionNum=128, --conf, spark.metrics.namespace=app_name:${spark.app.name}.app_id:${spark.app.id}, --conf, spark.maxRemoteBlockSizeFetchToMem=2147483135, --conf, spark.yarn.jars=local:/usr/lib/spark/jars/*, --conf, spark.default.parallelism=32, --conf, spark.sql.adaptive.enabled=true, --conf, spark.sql.cbo.joinReorder.enabled=true, --conf, spark.eventLog.dir=gs://dataproc-temp-us-central1-692577765454-wkt6hqvh/a708f90d-875b-4352-8a40-eb30c8f3dde8/spark-job-history, --conf, spark.yarn.historyServer.address=cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m:18080, --conf, spark.dynamicAllocation.minExecutors=1, --conf, spark.executor.instances=2, --conf, spark.history.fs.logDirectory=gs://dataproc-temp-us-central1-692577765454-wkt6hqvh/a708f90d-875b-4352-8a40-eb30c8f3dde8/spark-job-history, --conf, spark.executor.memory=2048m, --conf, spark.driver.memory=2048m, --conf, spark.rpc.message.maxSize=512, --conf, spark.driver.cores=1, --conf, spark.yarn.archive=hdfs://cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-m/framework/spark/spark.archive-spark2_2.11-3.2.3.zip, --conf, spark.yarn.maxAppAttempts=1, --conf, spark.scheduler.minRegisteredResourcesRatio=0.0, --conf, spark.executor.id=DemoBQ, --conf, spark.ui.port=0, --conf, spark.executor.cores=1, --conf, spark.metrics.conf=metrics.properties, --conf, spark.speculation=false, --conf, spark.dynamicAllocation.maxExecutors=10000, --conf, spark.executorEnv.OPENBLAS_NUM_THREADS=1, --conf, spark.sql.caseSensitive=true, --conf, spark.app.id=DemoBQ, --conf, spark.extraListeners=io.cdap.cdap.app.runtime.spark.DelegatingSparkListener,com.google.cloud.spark.performance.DataprocMetricsListener, --conf, spark.sql.autoBroadcastJoinThreshold=-1, --conf, spark.hadoop.hive.execution.engine=mr, --conf, spark.scheduler.mode=FAIR, --conf, spark.eventLog.enabled=true, --conf, spark.yarn.unmanagedAM.enabled=true, --conf, spark.shuffle.service.enabled=true, --conf, spark.sql.cbo.enabled=true, --conf, spark.executor.extraJavaOptions=-XX:+UseG1GC -verbose:gc -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1M -XX:+ExitOnOutOfMemoryError -Dstreaming.checkpoint.rewrite.enabled=true, --conf, spark.cdap.localized.resources=["HydratorSpark.config"], --conf, spark.dynamicAllocation.enabled=true, --conf, spark.driver.extraJavaOptions=-XX:+UseG1GC -verbose:gc -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1M -XX:+ExitOnOutOfMemoryError -Dstreaming.checkpoint.rewrite.enabled=true, --conf, spark.yarn.security.tokens.hbase.enabled=false, --conf, spark.yarn.security.tokens.hive.enabled=false, --conf, spark.executorEnv.CDAP_LOG_DIR=<LOG_DIR>, --conf, spark.yarn.appMasterEnv.CDAP_LOG_DIR=<LOG_DIR>, --archives, file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/program.jar.expanded.zip,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/artifacts_archive.jar,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/cdap-spark.jar, --files, file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/tmp/HydratorSpark7733714350962680849.config#HydratorSpark.config,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/program.jar,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/cdap-spark-launcher.jar,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/metrics.properties,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/cConf.xml,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/hConf.xml,file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/logback.xml.jar, --class, io.cdap.cdap.app.runtime.spark.SparkMainWrapper, /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/cdapSparkJob.jar, --cdap.spark.program=program_run:default.DemoBQ.-SNAPSHOT.spark.phase-1.f506beb1-13e2-11ed-8877-42010a800005, --cdap.user.main.class=io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver]
2022-08-04 10:48:33,348 - WARN  [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:48:34,051 - WARN  [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:48:34,451 - WARN  [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:o.a.s.d.y.c.package@422] - Can not load the default value of `spark.yarn.isHadoopProvided` from `org/apache/spark/deploy/yarn/config.properties` with error, java.lang.NullPointerException. Using `false` as a default value.
2022-08-04 10:48:40,415 - WARN  [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:o.a.s.d.s.HadoopDelegationTokenManager@69] - spark.yarn.security.tokens.hive.enabled is deprecated.  Please use spark.security.credentials.hive.enabled instead.
2022-08-04 10:48:40,430 - WARN  [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:o.a.s.d.s.HadoopDelegationTokenManager@69] - spark.yarn.security.tokens.hbase.enabled is deprecated.  Please use spark.security.credentials.hbase.enabled instead.
2022-08-04 10:48:57,179 - INFO  [main:i.c.c.a.r.s.d.SparkContainerLauncher@-2] - Launch main class org.apache.spark.deploy.yarn.ApplicationMaster.main([--class, io.cdap.cdap.app.runtime.spark.SparkMainWrapper, --jar, file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1659609855541_0001/container_1659609855541_0001_01_000002/data/tmp/1659610101777-0/cdapSparkJob.jar, --arg, --cdap.spark.program=program_run:default.DemoBQ.-SNAPSHOT.spark.phase-1.f506beb1-13e2-11ed-8877-42010a800005, --arg, --cdap.user.main.class=io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver, --properties-file, /hadoop/yarn/nm-local-dir/usercache/yarn/appcache/application_1659609855541_0002/container_1659609855541_0002_01_000001/__spark_conf__/__spark_conf__.properties, --dist-cache-conf, /hadoop/yarn/nm-local-dir/usercache/yarn/appcache/application_1659609855541_0002/container_1659609855541_0002_01_000001/__spark_conf__/__spark_dist_cache__.properties])
2022-08-04 10:48:59,327 - WARN  [main:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:02,755 - WARN  [Driver:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:02,890 - WARN  [Driver:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:02,898 - DEBUG [Driver:i.c.c.a.r.s.SparkRuntimeUtils@298] - Spark events log directory for program_run:default.DemoBQ.-SNAPSHOT.spark.phase-1.f506beb1-13e2-11ed-8877-42010a800005 is /hadoop/yarn/nm-local-dir/usercache/yarn/appcache/application_1659609855541_0002/container_1659609855541_0002_01_000001/tmp/spark-events3005599850796621718
2022-08-04 10:49:03,334 - WARN  [Driver:o.a.h.f.FileSystem@3304] - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HftpFileSystem not found
2022-08-04 10:49:03,337 - WARN  [Driver:o.a.h.f.FileSystem@3304] - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HsftpFileSystem not found
2022-08-04 10:49:04,503 - INFO  [SparkDriverHttpService STARTING:i.c.h.NettyHttpService@181] - Starting HTTP Service phase-1-http-service at address cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-w-0.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal/10.128.0.4:0
2022-08-04 10:49:04,685 - WARN  [Driver:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:05,167 - INFO  [SparkDriverService:i.c.c.a.r.s.d.SparkDriverService@102] - SparkDriverService started.
2022-08-04 10:49:05,180 - INFO  [Driver:i.c.c.a.r.s.SparkMainWrapper$@78] - Launching user spark class class io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver
2022-08-04 10:49:05,272 - WARN  [Driver:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:06,865 - INFO  [Driver:o.s.j.u.log@169] - Logging initialized @18894ms to org.sparkproject.jetty.util.log.Slf4jLog
2022-08-04 10:49:07,097 - INFO  [Driver:o.s.j.s.Server@375] - jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09
2022-08-04 10:49:07,143 - INFO  [Driver:o.s.j.s.Server@415] - Started @19171ms
2022-08-04 10:49:07,254 - INFO  [Driver:o.s.j.s.AbstractConnector@331] - Started ServerConnector@141d10bf{HTTP/1.1, (http/1.1)}{0.0.0.0:44821}
2022-08-04 10:49:07,346 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@1bcce087{/jobs,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,354 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@35c29ba8{/jobs/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,358 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@d6e6b2d{/jobs/job,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,366 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@324dd17c{/jobs/job/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,370 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@468efe38{/stages,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,378 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@959cc5f{/stages/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,388 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@30e5e043{/stages/stage,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,396 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@7771a493{/stages/stage/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,401 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@13d41a2f{/stages/pool,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,410 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@181d65cb{/stages/pool/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,418 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@5b24ea82{/storage,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,423 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@28230664{/storage/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,428 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@3f11a51{/storage/rdd,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,433 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@231d4ca7{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,440 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@7d6e808d{/environment,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,446 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@49338938{/environment/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,449 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@43014051{/executors,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,455 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@2cd87d6f{/executors/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,461 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@5d5f1963{/executors/threadDump,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,465 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@7dc86585{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,495 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@629d50ad{/static,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,500 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@1aede65d{/,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,504 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@641fac34{/api,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,508 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@10478509{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,511 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@4cf4f7a9{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-08-04 10:49:07,789 - WARN  [Driver:o.a.s.s.FairSchedulableBuilder@69] - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
2022-08-04 10:49:08,096 - DEBUG [Driver:i.c.c.a.r.s.SparkMetricsSink@51] - Using SparkMetricsSink for reporting metrics: {class=io.cdap.cdap.app.runtime.spark.SparkMetricsSink}
2022-08-04 10:49:08,122 - INFO  [Driver:o.s.j.s.h.ContextHandler@916] - Started o.s.j.s.ServletContextHandler@38b44171{/metrics/json,null,AVAILABLE,@Spark}
2022-08-04 10:49:09,823 - WARN  [dispatcher-event-loop-0:o.a.s.s.c.YarnSchedulerBackend$YarnSchedulerEndpoint@69] - Attempted to request executors before the AM has registered!
2022-08-04 10:49:10,215 - WARN  [main:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:11,056 - DEBUG [main:i.c.c.a.r.s.SparkMetricsSink@51] - Using SparkMetricsSink for reporting metrics: {class=io.cdap.cdap.app.runtime.spark.SparkMetricsSink}
2022-08-04 10:49:12,591 - WARN  [Driver:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:14,707 - INFO  [Driver:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Using output path 'gs://9b043284-cf75-4c64-aed1-0534a9534ed2/9b043284-cf75-4c64-aed1-0534a9534ed2/input/titanic-9b043284-cf75-4c64-aed1-0534a9534ed2'.
2022-08-04 10:49:15,125 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Bigquery connector version hadoop2-1.0.0
2022-08-04 10:49:15,134 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2022-08-04 10:49:15,147 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2022-08-04 10:49:15,152 - INFO  [Driver:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2022-08-04 10:49:15,551 - INFO  [Driver:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2022-08-04 10:49:15,793 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2022-08-04 10:49:15,805 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2022-08-04 10:49:15,808 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2022-08-04 10:49:15,816 - INFO  [Driver:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2022-08-04 10:49:16,666 - DEBUG [Driver:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable dataset.service with address dataset.service:0 and payload https://
2022-08-04 10:49:18,455 - DEBUG [spark-listener-group-shared:i.c.c.a.r.s.AbstractSparkExecutionContext@156] - Spark program=program:default.DemoBQ.-SNAPSHOT.spark.phase-1, runId=f506beb1-13e2-11ed-8877-42010a800005, jobId=0 starts with auto-commit=false on transaction Transaction{readPointer: 9223372036854775806, transactionId: 1659610151101000000, writePointer: 1659610151101000000, invalids: [], inProgress: [], firstShortInProgress: 9223372036854775807, type: LONG, checkpointWritePointers: [], visibilityLevel: SNAPSHOT}
2022-08-04 10:49:18,458 - DEBUG [spark-listener-group-shared:i.c.c.a.r.s.SparkTransactionHandler@110] - Spark job started: JobTransaction{jobId=0, stageIds=[0], transaction=Transaction{readPointer: 9223372036854775806, transactionId: 1659610151101000000, writePointer: 1659610151101000000, invalids: [], inProgress: [], firstShortInProgress: 9223372036854775807, type: LONG, checkpointWritePointers: [], visibilityLevel: SNAPSHOT}}
2022-08-04 10:49:25,389 - INFO  [main:i.c.c.a.r.s.d.SparkContainerLauncher@-2] - Launch main class org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main([--driver-url, spark://CoarseGrainedScheduler@cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-w-0.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal:38357, --executor-id, 1, --hostname, cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-w-0.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal, --cores, 1, --app-id, application_1659609855541_0002, --resourceProfileId, 0, --user-class-path, file:/hadoop/yarn/nm-local-dir/usercache/yarn/appcache/application_1659609855541_0002/container_1659609855541_0002_01_000002/__app__.jar])
2022-08-04 10:49:28,656 - WARN  [main:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:28,105 - INFO  [main:i.c.c.a.r.s.d.SparkContainerLauncher@-2] - Launch main class org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main([--driver-url, spark://CoarseGrainedScheduler@cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-w-0.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal:38357, --executor-id, 2, --hostname, cdap-demobq-1fc8e6b3-13e2-11ed-8c74-9e84527b297a-w-1.us-central1-a.c.qwiklabs-gcp-01-42fcbb83597b.internal, --cores, 1, --app-id, application_1659609855541_0002, --resourceProfileId, 0, --user-class-path, file:/hadoop/yarn/nm-local-dir/usercache/yarn/appcache/application_1659609855541_0002/container_1659609855541_0002_01_000003/__app__.jar])
2022-08-04 10:49:29,793 - DEBUG [main:i.c.c.a.r.s.SparkMetricsSink@51] - Using SparkMetricsSink for reporting metrics: {class=io.cdap.cdap.app.runtime.spark.SparkMetricsSink}
2022-08-04 10:49:31,398 - WARN  [main:o.a.s.SparkConf@69] - The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
2022-08-04 10:49:32,382 - DEBUG [main:i.c.c.a.r.s.SparkMetricsSink@51] - Using SparkMetricsSink for reporting metrics: {class=io.cdap.cdap.app.runtime.spark.SparkMetricsSink}
2022-08-04 10:49:37,242 - WARN  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):o.a.h.f.FileSystem@3304] - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HftpFileSystem not found
2022-08-04 10:49:37,256 - WARN  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):o.a.h.f.FileSystem@3304] - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HsftpFileSystem not found
2022-08-04 10:49:38,235 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2022-08-04 10:49:38,375 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.BigQueryFactory@76] - Bigquery connector version hadoop2-1.0.0
2022-08-04 10:49:38,378 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2022-08-04 10:49:38,387 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2022-08-04 10:49:38,390 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2022-08-04 10:49:38,398 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2022-08-04 10:49:38,403 - INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2022-08-04 10:49:39,523 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable svc.system.pipeline.studio with address svc.system.pipeline.studio:0 and payload https://
2022-08-04 10:49:41,148 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,187 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,195 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,229 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,246 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,257 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,270 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,272 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,278 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,298 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,304 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,306 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,308 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,312 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,336 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,353 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,385 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,388 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,421 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,426 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,430 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,444 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,451 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,461 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,468 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,476 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,478 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,481 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,487 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,493 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,495 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,497 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,501 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,504 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,516 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,535 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,548 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,552 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,556 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,568 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,571 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,587 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,596 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,616 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,630 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,633 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,648 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,655 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,678 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,685 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,688 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,697 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,716 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,734 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,740 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,742 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,750 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,757 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,758 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,769 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,776 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,783 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,791 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,799 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,805 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,808 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,816 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,827 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,831 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,833 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,835 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,839 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,842 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,844 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,848 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,850 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,867 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,874 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,876 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,879 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,880 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,892 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,905 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,919 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,922 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,924 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,931 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,937 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,938 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,946 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,955 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,958 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,975 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,979 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,981 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,986 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,988 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:41,995 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,001 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,004 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,007 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,019 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,026 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,029 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,031 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,037 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,040 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,046 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,048 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,053 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,054 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,059 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,068 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,071 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,078 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,084 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,086 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,089 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,097 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,111 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,119 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,124 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,129 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,131 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,133 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,139 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,142 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,148 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,156 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,160 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,166 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,171 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,173 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,175 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,181 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,184 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,190 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,197 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,202 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,207 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,210 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,211 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,214 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,218 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,225 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,227 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,231 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,241 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,245 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,251 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,254 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,255 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,262 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,264 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,269 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,273 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,287 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,303 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,307 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,309 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,319 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,325 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,329 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,334 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,336 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,346 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,353 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,357 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,363 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,376 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,380 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,387 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,395 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,401 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,408 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,413 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,415 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,418 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,426 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,429 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,432 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,438 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,440 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,441 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,446 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,454 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,456 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,465 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,468 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,470 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,476 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,477 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,486 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,488 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,491 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,494 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,496 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,500 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,513 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,524 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,528 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,546 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: empty ( Age )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:42,548 - DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0):i.c.w.e.RecipePipelineExecutor@135] - Error while applying directives
io.cdap.wrangler.api.ErrorRowException: ! dq :isNumber ( Age ) || ! dq :isInteger ( Age ) || ( Age == 0 || Age > 125 )  (ecode: 1, directive: send-to-error)
	at io.cdap.directives.row.SendToError.execute(SendToError.java:124) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.directives.row.SendToError.execute(SendToError.java:55) ~[wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:121) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.executor.RecipePipelineExecutor.execute(RecipePipelineExecutor.java:90) [wrangler-core-4.6.0.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:377) [%20artifact6240543053547981361.jar:na]
	at io.cdap.wrangler.Wrangler.transform(Wrangler.java:82) [%20artifact6240543053547981361.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.lambda$transform$5(WrappedTransform.java:90) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.Caller$1.call(Caller.java:30) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.StageLoggingCaller.call(StageLoggingCaller.java:40) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.plugin.WrappedTransform.transform(WrappedTransform.java:89) [cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.common.TrackedTransform.transform(TrackedTransform.java:74) ~[cdap-etl-core-6.6.0.jar:na]
	at io.cdap.cdap.etl.spark.function.TransformFunction.call(TransformFunction.java:54) ~[hydrator-spark-core2_2.11-6.6.0.jar:na]
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.14.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.3.jar:na]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501) ~[spark-core_2.12-3.1.3.jar:3.1.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) ~[na:1.8.0_332]
2022-08-04 10:49:44,747 - DEBUG [spark-listener-group-shared:i.c.c.a.r.s.SparkTransactionHandler@144] - Spark job ended: JobTransaction{jobId=0, stageIds=[0], transaction=Transaction{readPointer: 9223372036854775806, transactionId: 1659610151101000000, writePointer: 1659610151101000000, invalids: [], inProgress: [], firstShortInProgress: 9223372036854775807, type: LONG, checkpointWritePointers: [], visibilityLevel: SNAPSHOT}}
2022-08-04 10:49:45,779 - INFO  [Driver:i.c.p.g.b.s.BigQueryOutputFormat@272] - Importing into table 'qwiklabs-gcp-01-42fcbb83597b:demo_cdf.titanic' from 1 paths; path[0] is 'gs://9b043284-cf75-4c64-aed1-0534a9534ed2/9b043284-cf75-4c64-aed1-0534a9534ed2/input/titanic-9b043284-cf75-4c64-aed1-0534a9534ed2/part-r-00000.avro'; awaitCompletion: true
2022-08-04 10:49:45,789 - INFO  [Driver:i.c.p.g.b.s.BigQueryOutputFormat@376] - Using schema 'GenericData{classInfo=[fields], {fields=[{"mode":"NULLABLE","name":"PassengerId","type":"INTEGER"}, {"mode":"NULLABLE","name":"Survived","type":"STRING"}, {"mode":"NULLABLE","name":"Pclass","type":"STRING"}, {"mode":"NULLABLE","name":"Sex","type":"STRING"}, {"mode":"NULLABLE","name":"Age","type":"INTEGER"}, {"mode":"NULLABLE","name":"SibSp","type":"STRING"}, {"mode":"NULLABLE","name":"Parch","type":"STRING"}, {"mode":"NULLABLE","name":"Ticket","type":"STRING"}, {"mode":"NULLABLE","name":"Fare","type":"FLOAT"}, {"mode":"NULLABLE","name":"Cabin","type":"STRING"}, {"mode":"NULLABLE","name":"Embarked","type":"STRING"}, {"mode":"NULLABLE","name":"Last_Name","type":"STRING"}, {"mode":"NULLABLE","name":"Salutation","type":"STRING"}, {"mode":"NULLABLE","name":"First_Name","type":"STRING"}, {"mode":"NULLABLE","name":"Today_Fare","type":"FLOAT"}, {"mode":"NULLABLE","name":"id","type":"STRING"}]}}' for the load job config.
2022-08-04 10:50:00,121 - INFO  [Driver:i.c.p.g.b.s.BigQueryOutputFormat@409] - Imported into table 'qwiklabs-gcp-01-42fcbb83597b:demo_cdf.titanic' from 1 paths; path[0] is 'gs://9b043284-cf75-4c64-aed1-0534a9534ed2/9b043284-cf75-4c64-aed1-0534a9534ed2/input/titanic-9b043284-cf75-4c64-aed1-0534a9534ed2/part-r-00000.avro'
2022-08-04 10:50:00,191 - INFO  [Driver:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputCommitter@76] - Found GCS output data at 'gs://9b043284-cf75-4c64-aed1-0534a9534ed2/9b043284-cf75-4c64-aed1-0534a9534ed2/input/titanic-9b043284-cf75-4c64-aed1-0534a9534ed2', attempting to clean up.
2022-08-04 10:50:00,457 - INFO  [Driver:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputCommitter@76] - Successfully deleted GCS output path 'gs://9b043284-cf75-4c64-aed1-0534a9534ed2/9b043284-cf75-4c64-aed1-0534a9534ed2/input/titanic-9b043284-cf75-4c64-aed1-0534a9534ed2'.
2022-08-04 10:50:00,799 - INFO  [SparkDriverService:i.c.c.a.r.s.d.SparkDriverService@150] - SparkDriverService stopped.
2022-08-04 10:50:00,856 - INFO  [SparkDriverService:o.s.j.s.AbstractConnector@381] - Stopped Spark@141d10bf{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-04 10:50:00,932 - DEBUG [main:i.c.c.l.a.LogAppenderInitializer@137] - Stopping log appender TMSLogAppender
2022-08-04 10:50:00,964 - DEBUG [main:i.c.c.l.a.LogAppenderInitializer@137] - Stopping log appender TMSLogAppender
2022-08-04 10:50:01,826 - INFO  [SparkDriverHttpService STOPPING:i.c.h.NettyHttpService@258] - Stopping HTTP Service phase-1-http-service
2022-08-04 10:50:01,859 - DEBUG [SparkDriverService:i.c.c.i.a.r.d.r.RemoteExecutionDiscoveryService@142] - Update discoverable runtime with address r-cdf-lab-instance-qwiklabs-gcp-01-42fcbb83597b-dot-usc1.datafusion.googleusercontent.com/142.250.148.132:443 and payload https://
2022-08-04 10:50:03,274 - DEBUG [SparkDriverService:i.c.c.a.r.s.SparkRuntimeUtils@389] - Uploaded event logs file /hadoop/yarn/nm-local-dir/usercache/yarn/appcache/application_1659609855541_0002/container_1659609855541_0002_01_000001/tmp/spark-events3005599850796621718/1659610142901.lz4 for program run program_run:default.DemoBQ.-SNAPSHOT.spark.phase-1.f506beb1-13e2-11ed-8877-42010a800005
2022-08-04 10:50:03,287 - DEBUG [main:i.c.c.l.a.LogAppenderInitializer@137] - Stopping log appender TMSLogAppender
2022-08-04 10:50:03,898 - DEBUG [spark-submitter-phase-1-f506beb1-13e2-11ed-8877-42010a800005:i.c.c.a.r.s.s.AbstractSparkSubmitter@171] - SparkSubmit returned for program:default.DemoBQ.-SNAPSHOT.spark.phase-1 f506beb1-13e2-11ed-8877-42010a800005
2022-08-04 10:50:03,926 - INFO  [SparkExecutionService STOPPING:i.c.h.NettyHttpService@258] - Stopping HTTP Service phase-1-spark-exec-service
2022-08-04 10:50:04,946 - INFO  [SparkRunnerphase-1:i.c.p.g.b.s.BigQuerySink@139] - Job JobId{project=qwiklabs-gcp-01-42fcbb83597b, job=3cf5863c-f7a4-47de-a769-e9f21113ffce, location=US} affected 689 rows
2022-08-04 10:50:04,991 - DEBUG [SparkRunnerphase-1:i.c.c.a.r.s.SparkRuntimeService@901] - Running Spark shutdown hook org.apache.spark.util.SparkShutdownHookManager$$anon$2@2add167c
2022-08-04 10:50:05,085 - DEBUG [SparkRunnerphase-1:i.c.c.a.r.s.SparkRuntimeService@376] - Spark program completed: SparkRuntimeContext{id=program:default.DemoBQ.-SNAPSHOT.spark.phase-1, runId=f506beb1-13e2-11ed-8877-42010a800005}
2022-08-04 10:50:05,158 - INFO  [action-phase-1-0:i.c.c.i.a.r.w.WorkflowDriver@340] - Spark Program 'phase-1' in workflow completed
2022-08-04 10:50:05,184 - INFO  [WorkflowDriver:i.c.c.i.a.r.w.WorkflowDriver@631] - Workflow 'DataPipelineWorkflow' with run id '1fc8e6b3-13e2-11ed-8c74-9e84527b297a' completed
2022-08-04 10:50:05,187 - INFO  [WorkflowDriver:i.c.c.d.SmartWorkflow@563] - Pipeline 'DemoBQ' succeeded.
2022-08-04 10:50:05,326 - DEBUG [WorkflowDriver:i.c.c.e.l.FieldLineageProcessor@142] - The pipeline has redundant operations {Wrangler={First_Name=[operation_0, operation_10], Today_Fare=[operation_0], Last_Name=[operation_0], Salutation=[operation_0]}} and they will be ignored
2022-08-04 10:50:05,609 - DEBUG [WorkflowDriver:i.c.c.i.a.r.w.WorkflowProgramController@77] - Workflow service terminated from RUNNING. Un-registering service workflow.default.DemoBQ.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:50:05,621 - INFO  [TwillContainerService:i.c.c.i.a.r.d.AbstractProgramTwillRunnable@281] - Program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a completed. Releasing resources.
2022-08-04 10:50:05,624 - DEBUG [TwillContainerService:i.c.c.l.a.LogAppenderInitializer@137] - Stopping log appender TMSLogAppender
2022-08-04 10:50:11,237 - DEBUG [main:i.c.c.l.a.LogAppenderInitializer@137] - Stopping log appender TMSLogAppender
2022-08-04 10:50:13,559 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@125] - Executing DEPROVISION subtask REQUESTING_DELETE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:50:14,449 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@129] - Completed DEPROVISION subtask REQUESTING_DELETE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:50:44,952 - DEBUG [provisioning-task-4:i.c.c.i.p.t.ProvisioningTask@125] - Executing DEPROVISION subtask POLLING_DELETE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:50:45,962 - DEBUG [provisioning-task-4:i.c.c.i.p.t.ProvisioningTask@129] - Completed DEPROVISION subtask POLLING_DELETE for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
2022-08-04 10:50:45,980 - DEBUG [provisioning-task-4:i.c.c.i.p.t.ProvisioningTask@116] - Completed DEPROVISION task for program run program_run:default.DemoBQ.-SNAPSHOT.workflow.DataPipelineWorkflow.1fc8e6b3-13e2-11ed-8c74-9e84527b297a.
